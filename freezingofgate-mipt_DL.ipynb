{"cells":[{"cell_type":"markdown","metadata":{},"source":["# PyTorch Solution: Скользащие окна\n","Бэйзлан решение на основе анализа изменений показаний акселерометров во временом окне до и после события.\n"]},{"cell_type":"markdown","metadata":{},"source":["Данное решение намного хуже по качеству, чем решение с применением XGBoost, скорее всего потребует более многослойной обрабтки."]},{"cell_type":"markdown","metadata":{},"source":["Открытые данные для анализа - [70 Gb](https://www.kaggle.com/competitions/tlvmc-parkinsons-freezing-gait-prediction/data)"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-12-05T09:44:14.662947Z","iopub.status.busy":"2024-12-05T09:44:14.662590Z","iopub.status.idle":"2024-12-05T09:44:17.510928Z","shell.execute_reply":"2024-12-05T09:44:17.510141Z","shell.execute_reply.started":"2024-12-05T09:44:14.662916Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import random\n","import time\n","\n","import json\n","from tqdm import tqdm\n","import glob\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n","from sklearn.metrics import accuracy_score, average_precision_score\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:44:26.335149Z","iopub.status.busy":"2024-12-05T09:44:26.334183Z","iopub.status.idle":"2024-12-05T09:44:26.392623Z","shell.execute_reply":"2024-12-05T09:44:26.391543Z","shell.execute_reply.started":"2024-12-05T09:44:26.335108Z"},"trusted":true},"outputs":[],"source":["class Config:\n","    train_dir1 = \"/train/defog\"\n","    train_dir2 = \"/train/tdcsfog\"\n","\n","    batch_size = 1024\n","    window_size = 32\n","    window_future = 8\n","    window_past = window_size - window_future\n","    \n","    wx = 8\n","    \n","    model_dropout = 0.2\n","    model_hidden = 512\n","    model_nblocks = 3\n","    \n","    lr = 0.00015\n","    num_epochs = 8\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    \n","    feature_list = ['AccV', 'AccML', 'AccAP']\n","    label_list = ['StartHesitation', 'Turn', 'Walking']\n","    \n","    \n","cfg = Config()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:44:32.764202Z","iopub.status.busy":"2024-12-05T09:44:32.763845Z","iopub.status.idle":"2024-12-05T09:44:32.770911Z","shell.execute_reply":"2024-12-05T09:44:32.769911Z","shell.execute_reply.started":"2024-12-05T09:44:32.764170Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["cfg.device"]},{"cell_type":"markdown","metadata":{},"source":["# Стратификация Group K Fold\n","\n","1. **Структура данных:**\n","   - **Субъекты** (пациенты) в обучающем наборе данных и тестовом наборе различаются. Это означает, что нельзя использовать стандартное разбиение, так как важно исключить \"утечку\" данных между группами.\n","   - В тестовых данных есть разделение на публичную (public) и приватную (private) части, и между ними тоже нет пересечений субъектов.\n","\n","2. **Stratified Group K Fold:**\n","   - Это метод перекрестной проверки, который одновременно учитывает:\n","     - **Стратификацию:** Разбиение с сохранением пропорции классов (положительных и отрицательных примеров).\n","     - **Группы:** Гарантия, что данные от одного субъекта не попадают как в обучающий, так и в тестовый набор.\n","\n","3. **Редкость положительных примеров:**\n","   - В данных положительные примеры встречаются очень редко, что может привести к несбалансированным наборам данных после разбиения.\n","   - Поэтому важно **выбрать оптимальное разбиение**, чтобы положительные и отрицательные примеры были распределены максимально равномерно между фолдами.\n","\n","4. **Практическое применение:**\n","   - При создании Stratified Group K Fold необходимо заранее оценить, какое из разбиений обеспечивает наилучший баланс классов. Это можно сделать, проанализировав доли положительных/отрицательных примеров в каждом фолде."]},{"cell_type":"markdown","metadata":{},"source":["### tdcsfog"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:44:37.126230Z","iopub.status.busy":"2024-12-05T09:44:37.125544Z","iopub.status.idle":"2024-12-05T09:44:57.844750Z","shell.execute_reply":"2024-12-05T09:44:57.843770Z","shell.execute_reply.started":"2024-12-05T09:44:37.126196Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 833/833 [00:20<00:00, 40.40it/s]"]},{"name":"stdout","output_type":"stream","text":["32 files have positive values in all 3 classes\n","Fold = 0\n","Length of Train = 672, Length of Valid = 161\n","Train classes: 287,832, 1,462,652, 175,633\n","Valid classes: 16,958, 216,130, 32,205\n","Fold = 1\n","Length of Train = 613, Length of Valid = 220\n","Train classes: 51,748, 909,505, 65,242\n","Valid classes: 253,042, 769,277, 142,596\n","Fold = 2\n","Length of Train = 703, Length of Valid = 130\n","Train classes: 271,881, 1,332,746, 183,673\n","Valid classes: 32,909, 346,036, 24,165\n","Fold = 3\n","Length of Train = 649, Length of Valid = 184\n","Train classes: 303,710, 1,517,147, 205,196\n","Valid classes: 1,080, 161,635, 2,642\n","Fold = 4\n","Length of Train = 695, Length of Valid = 138\n","Train classes: 303,989, 1,493,078, 201,608\n","Valid classes: 801, 185,704, 6,230\n","Fold = 2\n","Length of Train = 703, Length of Valid = 130\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Основная цель состоит в том, чтобы выбрать наиболее сбалансированный фолд \n","# (часть данных) по количеству положительных примеров в разных классах.\n","\n","n1_sum = []\n","n2_sum = []\n","n3_sum = []\n","count = []\n","\n","metadata = pd.read_csv(\"tdcsfog_metadata.csv\")\n","\n","for f in tqdm(metadata['Id']):\n","    fpath = f\"train/tdcsfog/{f}.csv\"\n","    df = pd.read_csv(fpath)\n","    \n","    n1_sum.append(np.sum(df['StartHesitation']))\n","    n2_sum.append(np.sum(df['Turn']))\n","    n3_sum.append(np.sum(df['Walking']))\n","    count.append(len(df))\n","    \n","print(f\"32 files have positive values in all 3 classes\")\n","\n","metadata['n1_sum'] = n1_sum\n","metadata['n2_sum'] = n2_sum\n","metadata['n3_sum'] = n3_sum\n","metadata['count'] = count\n","\n","sgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\n","for i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n","    print(f\"Fold = {i}\")\n","    train_ids = metadata.loc[train_index, 'Id']\n","    valid_ids = metadata.loc[valid_index, 'Id']\n","    \n","    print(f\"Length of Train = {len(train_index)}, Length of Valid = {len(valid_index)}\")\n","    n1_sum = metadata.loc[train_index, 'n1_sum'].sum()\n","    n2_sum = metadata.loc[train_index, 'n2_sum'].sum()\n","    n3_sum = metadata.loc[train_index, 'n3_sum'].sum()\n","    print(f\"Train classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n","    \n","    n1_sum = metadata.loc[valid_index, 'n1_sum'].sum()\n","    n2_sum = metadata.loc[valid_index, 'n2_sum'].sum()\n","    n3_sum = metadata.loc[valid_index, 'n3_sum'].sum()\n","    print(f\"Valid classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n","    \n","\n","metadata = pd.read_csv(\"tdcsfog_metadata.csv\")\n","sgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\n","for i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n","    if i != 2:\n","        continue\n","    print(f\"Fold = {i}\")\n","    train_ids = metadata.loc[train_index, 'Id']\n","    valid_ids = metadata.loc[valid_index, 'Id']\n","    print(f\"Length of Train = {len(train_ids)}, Length of Valid = {len(valid_ids)}\")\n","    \n","    if i == 2:\n","        break\n","        \n","train_fpaths_tdcs = [f\"/train/tdcsfog/{_id}.csv\" for _id in train_ids]\n","valid_fpaths_tdcs = [f\"/train/tdcsfog/{_id}.csv\" for _id in valid_ids]"]},{"cell_type":"markdown","metadata":{},"source":["Выполнения анализа и создания разбиения данных с использованием **Stratified Group K Fold** (стратифицированной групповой перекрестной проверки). \n","\n","### Объяснение\n","\n","1. **Чтение метаданных**:\n","   - Считывается файл метаданных `tdcsfog_metadata.csv`, который содержит информацию о доступных данных.\n","\n","2. **Подсчет положительных примеров в каждом файле**:\n","   - Для каждого ID (`Id`) из метаданных открывается соответствующий файл с данными.\n","   - Для каждого файла подсчитываются суммы положительных значений в трех категориях: \n","     - `StartHesitation` — индикатор начала замедления.\n","     - `Turn` — индикатор поворота.\n","     - `Walking` — индикатор ходьбы.\n","   - Эти суммы добавляются в метаданные (`metadata`), чтобы потом анализировать распределение классов.\n","\n","3. **Создание Stratified Group K Fold**:\n","   - Используется `StratifiedGroupKFold` с 5 разбиениями.\n","     - **Стратификация**: Учитывается баланс классов.\n","     - **Группы**: Учитывается разделение субъектов (например, пациентов), чтобы данные одного субъекта не попадали одновременно в тренировочный и валидационный набор.\n","   - Для каждого фолда подсчитывается количество положительных примеров (`n1_sum`, `n2_sum`, `n3_sum`) в тренировочном и валидационном наборах, чтобы оценить, насколько сбалансировано разбиение.\n","\n","4. **Выбор оптимального фолда**:\n","   - Выбран **фолд №2** как наиболее сбалансированный, то есть с наилучшим распределением положительных и отрицательных примеров.\n","\n","5. **Финальное разбиение**:\n","   - После выбора фолда №2 из метаданных извлекаются идентификаторы для тренировочного (`train_ids`) и валидационного (`valid_ids`) наборов.\n","   - Генерируются пути к файлам для тренировочного и валидационного наборов.\n"]},{"cell_type":"markdown","metadata":{},"source":["Аналогично и для датасета defog"]},{"cell_type":"markdown","metadata":{},"source":["### defog"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:44:57.847313Z","iopub.status.busy":"2024-12-05T09:44:57.846839Z","iopub.status.idle":"2024-12-05T09:45:17.868658Z","shell.execute_reply":"2024-12-05T09:45:17.867716Z","shell.execute_reply.started":"2024-12-05T09:44:57.847256Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 137/137 [00:19<00:00,  6.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Fold = 0\n","Length of Train = 75, Length of Valid = 16\n","Train classes: 500, 428,683, 37,609\n","Valid classes: 0, 158,803, 60,910\n","Fold = 1\n","Length of Train = 65, Length of Valid = 26\n","Train classes: 216, 490,429, 84,955\n","Valid classes: 284, 97,057, 13,564\n","Fold = 2\n","Length of Train = 76, Length of Valid = 15\n","Train classes: 410, 488,634, 87,986\n","Valid classes: 90, 98,852, 10,533\n","Fold = 3\n","Length of Train = 70, Length of Valid = 21\n","Train classes: 435, 424,494, 88,800\n","Valid classes: 65, 162,992, 9,719\n","Fold = 4\n","Length of Train = 78, Length of Valid = 13\n","Train classes: 439, 517,704, 94,726\n","Valid classes: 61, 69,782, 3,793\n","Fold = 1\n","Length of Train = 65, Length of Valid = 26\n"]}],"source":["n1_sum = []\n","n2_sum = []\n","n3_sum = []\n","count = []\n","\n","metadata = pd.read_csv(\"defog_metadata.csv\")\n","metadata['n1_sum'] = 0\n","metadata['n2_sum'] = 0\n","metadata['n3_sum'] = 0\n","metadata['count'] = 0\n","\n","for f in tqdm(metadata['Id']):\n","    fpath = f\"/train/defog/{f}.csv\"\n","    if os.path.exists(fpath) == False:\n","        continue\n","        \n","    df = pd.read_csv(fpath)\n","    metadata.loc[metadata['Id'] == f, 'n1_sum'] = np.sum(df['StartHesitation'])\n","    metadata.loc[metadata['Id'] == f, 'n2_sum'] = np.sum(df['Turn'])\n","    metadata.loc[metadata['Id'] == f, 'n3_sum'] = np.sum(df['Walking'])\n","    metadata.loc[metadata['Id'] == f, 'count'] = len(df)\n","    \n","metadata = metadata[metadata['count'] > 0].reset_index()\n","\n","sgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\n","for i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n","    print(f\"Fold = {i}\")\n","    train_ids = metadata.loc[train_index, 'Id']\n","    valid_ids = metadata.loc[valid_index, 'Id']\n","    \n","    print(f\"Length of Train = {len(train_index)}, Length of Valid = {len(valid_index)}\")\n","    n1_sum = metadata.loc[train_index, 'n1_sum'].sum()\n","    n2_sum = metadata.loc[train_index, 'n2_sum'].sum()\n","    n3_sum = metadata.loc[train_index, 'n3_sum'].sum()\n","    print(f\"Train classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n","    \n","    n1_sum = metadata.loc[valid_index, 'n1_sum'].sum()\n","    n2_sum = metadata.loc[valid_index, 'n2_sum'].sum()\n","    n3_sum = metadata.loc[valid_index, 'n3_sum'].sum()\n","    print(f\"Valid classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n","    \n","sgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\n","for i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n","    if i != 1:\n","        continue\n","    print(f\"Fold = {i}\")\n","    train_ids = metadata.loc[train_index, 'Id']\n","    valid_ids = metadata.loc[valid_index, 'Id']\n","    print(f\"Length of Train = {len(train_ids)}, Length of Valid = {len(valid_ids)}\")\n","    \n","    if i == 2:\n","        break\n","        \n","train_fpaths_de = [f\"/train/defog/{_id}.csv\" for _id in train_ids]\n","valid_fpaths_de = [f\"/train/defog/{_id}.csv\" for _id in valid_ids]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:45:17.870017Z","iopub.status.busy":"2024-12-05T09:45:17.869765Z","iopub.status.idle":"2024-12-05T09:45:17.875143Z","shell.execute_reply":"2024-12-05T09:45:17.874098Z","shell.execute_reply.started":"2024-12-05T09:45:17.869993Z"},"trusted":true},"outputs":[],"source":["train_fpaths = [(f, 'de') for f in train_fpaths_de] + [(f, 'tdcs') for f in train_fpaths_tdcs]\n","valid_fpaths = [(f, 'de') for f in valid_fpaths_de] + [(f, 'tdcs') for f in valid_fpaths_tdcs]"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset\n","\n","При формировании датасета для конкретного временного момента используется **окно**, которое включает в себя значения акселерометра (**Acc**) как из **прошлого**, так и из **будущего** относительно этого временного момента.  \n","\n","Если какая-то часть данных из этого окна недоступна (например, из-за отсутствия данных или выхода за границы доступного временного диапазона), эти отсутствующие значения заменяются на **нули** (**padding**).  \n","\n","---\n","\n","1. **Окно (window):**\n","   - Это подмножество данных, которое включает несколько последовательных временных шагов, расположенных до и после текущего временного момента.\n","   - Например, если окно равно 5, это может означать 2 значения из прошлого, текущее значение и 2 значения из будущего.\n","\n","2. **Проблема отсутствующих данных:**\n","   - Для начала или конца временного ряда данные за прошлые или будущие временные шаги могут быть недоступны.\n","   - В таких случаях \"заполнение\" окна нулями позволяет сохранить его фиксированный размер.\n","\n","3. **Padding с нулями:**\n","   - Это стандартная техника в обработке временных рядов и последовательностей.\n","   - Задача заполнения нулями — избежать разрыва данных и обеспечить одинаковую структуру входов модели, независимо от того, есть ли полный контекст окна.\n","\n","---\n","\n","Использование таких окон помогает модели учитывать контекст временного ряда, что может улучшить качество прогнозов, особенно для временных последовательностей, таких как данные акселерометра."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:45:17.877024Z","iopub.status.busy":"2024-12-05T09:45:17.876780Z","iopub.status.idle":"2024-12-05T09:45:17.891400Z","shell.execute_reply":"2024-12-05T09:45:17.890541Z","shell.execute_reply.started":"2024-12-05T09:45:17.877001Z"},"trusted":true},"outputs":[],"source":["class FOGDataset(Dataset):\n","    def __init__(self, fpaths, scale=9.806, split=\"train\"):\n","        super(FOGDataset, self).__init__()\n","        tm = time.time()\n","        self.split = split\n","        self.scale = scale\n","        \n","        self.fpaths = fpaths\n","        self.dfs = [self.read(f[0], f[1]) for f in fpaths]\n","        self.f_ids = [os.path.basename(f[0])[:-4] for f in self.fpaths]\n","        \n","        self.end_indices = []\n","        self.shapes = []\n","        _length = 0\n","        for df in self.dfs:\n","            self.shapes.append(df.shape[0])\n","            _length += df.shape[0]\n","            self.end_indices.append(_length)\n","        \n","        self.dfs = np.concatenate(self.dfs, axis=0).astype(np.float16)\n","        self.length = self.dfs.shape[0]\n","        \n","        shape1 = self.dfs.shape[1]\n","        \n","        self.dfs = np.concatenate([np.zeros((cfg.wx*cfg.window_past, shape1)), self.dfs, np.zeros((cfg.wx*cfg.window_future, shape1))], axis=0)\n","        print(f\"Dataset initialized in {time.time() - tm} secs!\")\n","        gc.collect()\n","        \n","    def read(self, f, _type):\n","        df = pd.read_csv(f)\n","        if self.split == \"test\":\n","            return np.array(df)\n","        \n","        if _type ==\"tdcs\":\n","            df['Valid'] = 1\n","            df['Task'] = 1\n","            df['tdcs'] = 1\n","        else:\n","            df['tdcs'] = 0\n","        \n","        return np.array(df)\n","            \n","    def __getitem__(self, index):\n","        if self.split == \"train\":\n","            row_idx = random.randint(0, self.length-1) + cfg.wx*cfg.window_past\n","        elif self.split == \"test\":\n","            for i,e in enumerate(self.end_indices):\n","                if index >= e:\n","                    continue\n","                df_idx = i\n","                break\n","\n","            row_idx_true = self.shapes[df_idx] - (self.end_indices[df_idx] - index)\n","            _id = self.f_ids[df_idx] + \"_\" + str(row_idx_true)\n","            row_idx = index + cfg.wx*cfg.window_past\n","        else:\n","            row_idx = index + cfg.wx*cfg.window_past\n","            \n","        #scale = 9.806 if self.dfs[row_idx, -1] == 1 else 1.0\n","        x = self.dfs[row_idx - cfg.wx*cfg.window_past : row_idx + cfg.wx*cfg.window_future, 1:4]\n","        x = x[::cfg.wx, :][::-1, :]\n","        x = torch.tensor(x.astype('float'))#/scale\n","        \n","        t = self.dfs[row_idx, -3]*self.dfs[row_idx, -2]\n","        \n","        if self.split == \"test\":\n","            return _id, x, t\n","        \n","        y = self.dfs[row_idx, 4:7].astype('float')\n","        y = torch.tensor(y)\n","        \n","        return x, y, t\n","    \n","    def __len__(self):\n","        # return self.length\n","        if self.split == \"train\":\n","            return 5_000_000\n","        return self.length"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:45:17.892804Z","iopub.status.busy":"2024-12-05T09:45:17.892471Z","iopub.status.idle":"2024-12-05T09:45:18.031501Z","shell.execute_reply":"2024-12-05T09:45:18.030592Z","shell.execute_reply.started":"2024-12-05T09:45:17.892770Z"},"trusted":true},"outputs":[{"data":{"text/plain":["23"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:45:18.032899Z","iopub.status.busy":"2024-12-05T09:45:18.032673Z","iopub.status.idle":"2024-12-05T09:45:18.044563Z","shell.execute_reply":"2024-12-05T09:45:18.043696Z","shell.execute_reply.started":"2024-12-05T09:45:18.032878Z"},"trusted":true},"outputs":[],"source":["def _block(in_features, out_features, drop_rate):\n","    return nn.Sequential(\n","        nn.Linear(in_features, out_features),\n","        nn.BatchNorm1d(out_features),\n","        nn.ReLU(),\n","        nn.Dropout(drop_rate)\n","    )\n","\n","class FOGModel(nn.Module):\n","    def __init__(self, p=cfg.model_dropout, dim=cfg.model_hidden, nblocks=cfg.model_nblocks):\n","        super(FOGModel, self).__init__()\n","        self.dropout = nn.Dropout(p)\n","        self.in_layer = nn.Linear(cfg.window_size*3, dim)\n","        self.blocks = nn.Sequential(*[_block(dim, dim, p) for _ in range(nblocks)])\n","        self.out_layer = nn.Linear(dim, 3)\n","        \n","    def forward(self, x):\n","        x = x.view(-1, cfg.window_size*3)\n","        x = self.in_layer(x)\n","        for block in self.blocks:\n","            x = block(x)\n","        x = self.out_layer(x)\n","        return x"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:45:18.045879Z","iopub.status.busy":"2024-12-05T09:45:18.045613Z","iopub.status.idle":"2024-12-05T09:45:18.059247Z","shell.execute_reply":"2024-12-05T09:45:18.058383Z","shell.execute_reply.started":"2024-12-05T09:45:18.045856Z"},"trusted":true},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:45:21.507435Z","iopub.status.busy":"2024-12-05T09:45:21.507073Z","iopub.status.idle":"2024-12-05T09:45:21.519910Z","shell.execute_reply":"2024-12-05T09:45:21.518901Z","shell.execute_reply.started":"2024-12-05T09:45:21.507402Z"},"trusted":true},"outputs":[],"source":["from torch.cuda.amp import GradScaler\n","\n","def train_one_epoch(model, loader, optimizer, criterion):\n","    loss_sum = 0.\n","    scaler = GradScaler()\n","    \n","    model.train()\n","    for x,y,t in tqdm(loader):\n","        x = x.to(cfg.device).float()\n","        y = y.to(cfg.device).float()\n","        t = t.to(cfg.device).float()\n","        \n","        y_pred = model(x)\n","        loss = criterion(y_pred, y)\n","        loss = torch.mean(loss*t.unsqueeze(-1), dim=1)\n","        \n","        t_sum = torch.sum(t)\n","        if t_sum > 0:\n","            loss = torch.sum(loss)/t_sum\n","        else:\n","            loss = torch.sum(loss)*0.\n","        \n","        # loss.backward()\n","        scaler.scale(loss).backward()\n","        # optimizer.step()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","        optimizer.zero_grad()\n","        \n","        loss_sum += loss.item()\n","    \n","    print(f\"Train Loss: {(loss_sum/len(loader)):.04f}\")\n","    \n","\n","def validation_one_epoch(model, loader, criterion):\n","    loss_sum = 0.\n","    y_true_epoch = []\n","    y_pred_epoch = []\n","    t_valid_epoch = []\n","    \n","    model.eval()\n","    for x,y,t in tqdm(loader):\n","        x = x.to(cfg.device).float()\n","        y = y.to(cfg.device).float()\n","        t = t.to(cfg.device).float()\n","        \n","        with torch.no_grad():\n","            y_pred = model(x)\n","            loss = criterion(y_pred, y)\n","            loss = torch.mean(loss*t.unsqueeze(-1), dim=1)\n","            \n","            t_sum = torch.sum(t)\n","            if t_sum > 0:\n","                loss = torch.sum(loss)/t_sum\n","            else:\n","                loss = torch.sum(loss)*0.\n","        \n","        loss_sum += loss.item()\n","        y_true_epoch.append(y.cpu().numpy())\n","        y_pred_epoch.append(y_pred.cpu().numpy())\n","        t_valid_epoch.append(t.cpu().numpy())\n","        \n","    y_true_epoch = np.concatenate(y_true_epoch, axis=0)\n","    y_pred_epoch = np.concatenate(y_pred_epoch, axis=0)\n","    \n","    t_valid_epoch = np.concatenate(t_valid_epoch, axis=0)\n","    y_true_epoch = y_true_epoch[t_valid_epoch > 0, :]\n","    y_pred_epoch = y_pred_epoch[t_valid_epoch > 0, :]\n","    \n","    scores = [average_precision_score(y_true_epoch[:,i], y_pred_epoch[:,i]) for i in range(3)]\n","    mean_score = np.mean(scores)\n","    print(f\"Validation Loss: {(loss_sum/len(loader)):.04f}, Validation Score: {mean_score:.03f}, ClassWise: {scores[0]:.03f},{scores[1]:.03f},{scores[2]:.03f}\")\n","    \n","    return mean_score"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T09:45:26.347954Z","iopub.status.busy":"2024-12-05T09:45:26.347603Z","iopub.status.idle":"2024-12-05T10:03:48.860081Z","shell.execute_reply":"2024-12-05T10:03:48.859038Z","shell.execute_reply.started":"2024-12-05T09:45:26.347924Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters in model - 842,243\n","Dataset initialized in 29.09773302078247 secs!\n","Dataset initialized in 8.694570302963257 secs!\n","lengths of datasets: train - 5000000, valid - 4984740\n","==================================================\n","Epoch: 0\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4883/4883 [01:13<00:00, 66.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.1765\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4868/4868 [00:58<00:00, 83.84it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0888, Validation Score: 0.227, ClassWise: 0.037,0.618,0.027\n","Saving Model ...\n","==================================================\n","Epoch: 1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4883/4883 [01:10<00:00, 68.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.1539\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4868/4868 [00:57<00:00, 84.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0870, Validation Score: 0.244, ClassWise: 0.037,0.653,0.042\n","Saving Model ...\n","==================================================\n","Epoch: 2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4883/4883 [01:10<00:00, 68.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.1455\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4868/4868 [00:58<00:00, 83.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0864, Validation Score: 0.242, ClassWise: 0.040,0.649,0.036\n","==================================================\n","Epoch: 3\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4883/4883 [01:11<00:00, 68.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.1403\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4868/4868 [00:58<00:00, 83.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0957, Validation Score: 0.235, ClassWise: 0.036,0.629,0.039\n","==================================================\n","Epoch: 4\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4883/4883 [01:12<00:00, 67.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.1357\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4868/4868 [00:59<00:00, 81.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0941, Validation Score: 0.248, ClassWise: 0.034,0.669,0.043\n","Saving Model ...\n","==================================================\n","Epoch: 5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4883/4883 [01:11<00:00, 67.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.1323\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4868/4868 [00:59<00:00, 81.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0926, Validation Score: 0.247, ClassWise: 0.039,0.655,0.049\n","==================================================\n","Epoch: 6\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4883/4883 [01:11<00:00, 67.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.1294\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4868/4868 [00:58<00:00, 82.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.1014, Validation Score: 0.245, ClassWise: 0.030,0.656,0.050\n","==================================================\n","Epoch: 7\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4883/4883 [01:11<00:00, 68.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.1263\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4868/4868 [00:58<00:00, 82.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0891, Validation Score: 0.252, ClassWise: 0.046,0.667,0.043\n","Saving Model ...\n","==================================================\n"]},{"data":{"text/plain":["0"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["model = FOGModel().to(cfg.device)\n","print(f\"Number of parameters in model - {count_parameters(model):,}\")\n","\n","train_dataset = FOGDataset(train_fpaths, split=\"train\")\n","valid_dataset = FOGDataset(valid_fpaths, split=\"valid\")\n","print(f\"lengths of datasets: train - {len(train_dataset)}, valid - {len(valid_dataset)}\")\n","\n","train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, num_workers=5, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=cfg.batch_size, num_workers=5)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n","criterion = torch.nn.BCEWithLogitsLoss(reduction='none').to(cfg.device)\n","# sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.85)\n","\n","max_score = 0.0\n","\n","print(\"=\"*50)\n","for epoch in range(cfg.num_epochs):\n","    print(f\"Epoch: {epoch}\")\n","    train_one_epoch(model, train_loader, optimizer, criterion)\n","    score = validation_one_epoch(model, valid_loader, criterion)\n","    # sched.step()\n","\n","    if score > max_score:\n","        max_score = score\n","        torch.save(model.state_dict(), \"best_model_state.h5\")\n","        print(\"Saving Model ...\")\n","\n","    print(\"=\"*50)\n","    \n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Инференс"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T10:03:48.862697Z","iopub.status.busy":"2024-12-05T10:03:48.862215Z","iopub.status.idle":"2024-12-05T10:03:51.841557Z","shell.execute_reply":"2024-12-05T10:03:51.840355Z","shell.execute_reply.started":"2024-12-05T10:03:48.862659Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset initialized in 0.40098047256469727 secs!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 280/280 [00:02<00:00, 116.85it/s]\n"]}],"source":["model = FOGModel().to(cfg.device)\n","model.load_state_dict(torch.load(\"best_model_state.h5\"))\n","model.eval()\n","\n","test_defog_paths = glob.glob(\"/test/defog/*.csv\")\n","test_tdcsfog_paths = glob.glob(\"/test/tdcsfog/*.csv\")\n","test_fpaths = [(f, 'de') for f in test_defog_paths] + [(f, 'tdcs') for f in test_tdcsfog_paths]\n","\n","test_dataset = FOGDataset(test_fpaths, split=\"test\")\n","test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, num_workers=5)\n","\n","ids = []\n","preds = []\n","\n","for _id, x, _ in tqdm(test_loader):\n","    x = x.to(cfg.device).float()\n","    with torch.no_grad():\n","        y_pred = model(x)*0.1\n","    \n","    ids.extend(_id)\n","    preds.extend(list(np.nan_to_num(y_pred.cpu().numpy())))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T10:05:14.673178Z","iopub.status.busy":"2024-12-05T10:05:14.672797Z","iopub.status.idle":"2024-12-05T10:05:14.938943Z","shell.execute_reply":"2024-12-05T10:05:14.937926Z","shell.execute_reply.started":"2024-12-05T10:05:14.673143Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(286370, 4)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["sample_submission = pd.read_csv(\"sample_submission.csv\")\n","sample_submission.shape"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T10:05:19.275614Z","iopub.status.busy":"2024-12-05T10:05:19.274923Z","iopub.status.idle":"2024-12-05T10:05:20.390451Z","shell.execute_reply":"2024-12-05T10:05:20.389594Z","shell.execute_reply.started":"2024-12-05T10:05:19.275579Z"},"trusted":true},"outputs":[],"source":["preds = np.array(preds)\n","submission = pd.DataFrame({'Id': ids, 'StartHesitation': np.round(preds[:,0],5), \\\n","                           'Turn': np.round(preds[:,1],5), 'Walking': np.round(preds[:,2],5)})\n","\n","submission = pd.merge(sample_submission[['Id']], submission, how='left', on='Id').fillna(0.0)\n","submission.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-12-05T10:05:23.369858Z","iopub.status.busy":"2024-12-05T10:05:23.369097Z","iopub.status.idle":"2024-12-05T10:05:23.384862Z","shell.execute_reply":"2024-12-05T10:05:23.383947Z","shell.execute_reply.started":"2024-12-05T10:05:23.369822Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(286370, 4)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>StartHesitation</th>\n","      <th>Turn</th>\n","      <th>Walking</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>003f117e14_0</td>\n","      <td>-1.19135</td>\n","      <td>-1.47868</td>\n","      <td>-2.01220</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>003f117e14_1</td>\n","      <td>-1.19144</td>\n","      <td>-1.47808</td>\n","      <td>-2.01109</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>003f117e14_2</td>\n","      <td>-1.18948</td>\n","      <td>-1.47628</td>\n","      <td>-2.00929</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>003f117e14_3</td>\n","      <td>-1.19038</td>\n","      <td>-1.47931</td>\n","      <td>-2.01251</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>003f117e14_4</td>\n","      <td>-1.19100</td>\n","      <td>-1.47935</td>\n","      <td>-2.01252</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Id  StartHesitation     Turn  Walking\n","0  003f117e14_0         -1.19135 -1.47868 -2.01220\n","1  003f117e14_1         -1.19144 -1.47808 -2.01109\n","2  003f117e14_2         -1.18948 -1.47628 -2.00929\n","3  003f117e14_3         -1.19038 -1.47931 -2.01251\n","4  003f117e14_4         -1.19100 -1.47935 -2.01252"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["print(submission.shape)\n","submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":5677426,"sourceId":41880,"sourceType":"competition"},{"sourceId":121731966,"sourceType":"kernelVersion"}],"dockerImageVersionId":30408,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
